{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from dataset import prepare_dataset\n",
    "from dataset import ToFloatTensorInZeroOne\n",
    "from model import LSTM_with_EFFICIENTNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time() -> str:\n",
    "    return time.strftime('%c', time.localtime(time.time()))\n",
    "\n",
    "def clear_pycache(root: str) -> None:\n",
    "    if os.path.exists(os.path.join(root, '__pycache__')):\n",
    "        shutil.rmtree(os.path.join(root, '__pycache__'))\n",
    "\n",
    "def clear_log_folders(root: str) -> None:\n",
    "    if os.path.exists(os.path.join(root, 'checkpoints')):\n",
    "        shutil.rmtree(os.path.join(root, 'checkpoints'))\n",
    "    if os.path.exists(os.path.join(root, 'history')):\n",
    "        shutil.rmtree(os.path.join(root, 'history'))\n",
    "    if os.path.exists(os.path.join(root, 'results')):\n",
    "        shutil.rmtree(os.path.join(root, 'results'))\n",
    "\n",
    "# For updating learning rate\n",
    "def update_learning_rate(optimizer, lr) -> None:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True when using Google Colab\n",
    "colab = False\n",
    "\n",
    "    # Consider GPU memory limit\n",
    "    # Paper suggested 512\n",
    "    # Suggest 128 for GTX 1660 Ti\n",
    "batch_size = 16\n",
    "\n",
    "    # Last checkpoint's training position\n",
    "done_epochs = 0\n",
    "\n",
    "    # Consider Google Colab time limit\n",
    "    # How much epochs to train now\n",
    "train_epochs = 0\n",
    " \n",
    "clear_log = False\n",
    "prepare_dataset(colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset | Data preparation start @ Sat Feb 25 03:25:51 2023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241a42e0d7374924b33c22b39476afd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576bfe22cc604f69901db3296f245608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if colab:\n",
    "    root = '/content/drive/MyDrive'\n",
    "else:\n",
    "    root = './'\n",
    "\n",
    "if clear_log:\n",
    "    clear_log_folders(root)\n",
    "\n",
    "######## Preparing Dataset ########\n",
    "print(f\"Dataset | Data preparation start @ {get_time()}\", flush=True)\n",
    "\n",
    "timestamp = get_time().replace(':', '')\n",
    "# timestamp = 'Tue Feb 14 191746 2023'\n",
    "\n",
    "location = {\n",
    "    'video_path': os.path.join(root, '../datasets/hmdb51dataset/video'),\n",
    "    'annotation_path': os.path.join(root, '../datasets/hmdb51dataset/annotation'),\n",
    "    'checkpoints_path': os.path.join(root, 'checkpoints', timestamp),\n",
    "    'history_path': os.path.join(root, 'history', timestamp),\n",
    "    'results_path': os.path.join(root, 'results', timestamp)\n",
    "}\n",
    "if not os.path.exists(location['checkpoints_path']):\n",
    "    os.makedirs(location['checkpoints_path'])\n",
    "\n",
    "if not os.path.exists(location['history_path']):\n",
    "    os.makedirs(location['history_path'])\n",
    "\n",
    "if not os.path.exists(location['results_path']):\n",
    "    os.makedirs(location['results_path'])\n",
    "\n",
    "# Preprocessing dataset\n",
    "transform_train = transforms.Compose([\n",
    "    ToFloatTensorInZeroOne(),\n",
    "    transforms.Resize([256,342]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomCrop(224)\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    ToFloatTensorInZeroOne(),\n",
    "    transforms.Resize([256,342]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "dataset_train_val = torchvision.datasets.HMDB51(\n",
    "    root=location['video_path'],\n",
    "    annotation_path=location['annotation_path'],\n",
    "    frames_per_clip=10,\n",
    "    step_between_clips=5,\n",
    "    train=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "dataset_test = torchvision.datasets.HMDB51(\n",
    "    root=location['video_path'],\n",
    "    annotation_path=location['annotation_path'],\n",
    "    frames_per_clip=10,\n",
    "    step_between_clips=5,\n",
    "    train=False,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "# Train set 52.5%, validation set 17.5%, test set 30%\n",
    "dataset_len = len(dataset_train_val)\n",
    "train_len = math.floor(dataset_len * 0.75)\n",
    "val_len = dataset_len - train_len\n",
    "dataset_train, dataset_val = random_split(dataset_train_val, [train_len, val_len])\n",
    "\n",
    "# Loading dataset\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "loader_val = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "    dataset=dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "train_batches = len(loader_train)\n",
    "val_batches = len(loader_val)\n",
    "test_batches = len(loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM_with_EFFICIENTNET' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m######## Model & Hyperparameters ########\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m LSTM_with_EFFICIENTNET(num_classes\u001b[39m=\u001b[39m\u001b[39m51\u001b[39m,hidden_size\u001b[39m=\u001b[39m\u001b[39m51\u001b[39m,num_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,fine_tune\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m      5\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTM_with_EFFICIENTNET' is not defined"
     ]
    }
   ],
   "source": [
    "######## Model & Hyperparameters ########\n",
    "model = LSTM_with_EFFICIENTNET(num_classes=51,hidden_size=51,num_layers=2,pretrained=True,fine_tune=False).to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

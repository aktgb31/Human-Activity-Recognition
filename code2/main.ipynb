{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from dataset import prepare_dataset\n",
    "from dataset import ToFloatTensorInZeroOne\n",
    "from model import LSTM_with_EFFICIENTNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time() -> str:\n",
    "    return time.strftime('%c', time.localtime(time.time()))\n",
    "\n",
    "def clear_pycache(root: str) -> None:\n",
    "    if os.path.exists(os.path.join(root, '__pycache__')):\n",
    "        shutil.rmtree(os.path.join(root, '__pycache__'))\n",
    "\n",
    "def clear_log_folders(root: str) -> None:\n",
    "    if os.path.exists(os.path.join(root, 'checkpoints')):\n",
    "        shutil.rmtree(os.path.join(root, 'checkpoints'))\n",
    "    if os.path.exists(os.path.join(root, 'history')):\n",
    "        shutil.rmtree(os.path.join(root, 'history'))\n",
    "    if os.path.exists(os.path.join(root, 'results')):\n",
    "        shutil.rmtree(os.path.join(root, 'results'))\n",
    "\n",
    "# For updating learning rate\n",
    "def update_learning_rate(optimizer, lr) -> None:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True when using Google Colab\n",
    "colab = False\n",
    "\n",
    "    # Consider GPU memory limit\n",
    "    # Paper suggested 512\n",
    "    # Suggest 128 for GTX 1660 Ti\n",
    "batch_size = 16\n",
    "\n",
    "    # Last checkpoint's training position\n",
    "done_epochs = 0\n",
    "\n",
    "    # Consider Google Colab time limit\n",
    "    # How much epochs to train now\n",
    "train_epochs = 0\n",
    " \n",
    "clear_log = False\n",
    "prepare_dataset(colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset | Data preparation start @ Sat Feb 25 03:24:37 2023\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/video'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     27\u001b[0m transform_train \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     28\u001b[0m     ToFloatTensorInZeroOne(),\n\u001b[1;32m     29\u001b[0m     transforms\u001b[39m.\u001b[39mResize([\u001b[39m128\u001b[39m, \u001b[39m171\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     transforms\u001b[39m.\u001b[39mRandomCrop(\u001b[39m112\u001b[39m)\n\u001b[1;32m     33\u001b[0m ])\n\u001b[1;32m     34\u001b[0m transform_test \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     35\u001b[0m     ToFloatTensorInZeroOne(),\n\u001b[1;32m     36\u001b[0m     transforms\u001b[39m.\u001b[39mResize([\u001b[39m128\u001b[39m, \u001b[39m171\u001b[39m]),\n\u001b[1;32m     37\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m]),\n\u001b[1;32m     38\u001b[0m     transforms\u001b[39m.\u001b[39mCenterCrop(\u001b[39m112\u001b[39m)\n\u001b[1;32m     39\u001b[0m ])\n\u001b[0;32m---> 41\u001b[0m dataset_train_val \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mHMDB51(\n\u001b[1;32m     42\u001b[0m     root\u001b[39m=\u001b[39;49mlocation[\u001b[39m'\u001b[39;49m\u001b[39mvideo_path\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     43\u001b[0m     annotation_path\u001b[39m=\u001b[39;49mlocation[\u001b[39m'\u001b[39;49m\u001b[39mannotation_path\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     44\u001b[0m     frames_per_clip\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     45\u001b[0m     step_between_clips\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     46\u001b[0m     train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     47\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform_train\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m dataset_test \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mHMDB51(\n\u001b[1;32m     50\u001b[0m     root\u001b[39m=\u001b[39mlocation[\u001b[39m'\u001b[39m\u001b[39mvideo_path\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     51\u001b[0m     annotation_path\u001b[39m=\u001b[39mlocation[\u001b[39m'\u001b[39m\u001b[39mannotation_path\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     transform\u001b[39m=\u001b[39mtransform_test\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[39m# Train set 52.5%, validation set 17.5%, test set 30%\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/hmdb51.py:83\u001b[0m, in \u001b[0;36mHMDB51.__init__\u001b[0;34m(self, root, annotation_path, frames_per_clip, step_between_clips, frame_rate, fold, train, transform, _precomputed_metadata, num_workers, _video_width, _video_height, _video_min_dimension, _audio_samples, output_format)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfold should be between 1 and 3, got \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m extensions \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mavi\u001b[39m\u001b[39m\"\u001b[39m,)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses, class_to_idx \u001b[39m=\u001b[39m find_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples \u001b[39m=\u001b[39m make_dataset(\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot,\n\u001b[1;32m     86\u001b[0m     class_to_idx,\n\u001b[1;32m     87\u001b[0m     extensions,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m video_paths \u001b[39m=\u001b[39m [path \u001b[39mfor\u001b[39;00m (path, _) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/video'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if colab:\n",
    "    root = '/content/drive/MyDrive'\n",
    "else:\n",
    "    root = './'\n",
    "\n",
    "if clear_log:\n",
    "    clear_log_folders(root)\n",
    "\n",
    "######## Preparing Dataset ########\n",
    "print(f\"Dataset | Data preparation start @ {get_time()}\", flush=True)\n",
    "\n",
    "timestamp = get_time().replace(':', '')\n",
    "# timestamp = 'Tue Feb 14 191746 2023'\n",
    "\n",
    "location = {\n",
    "    'video_path': os.path.join(root, '../datasets/hmdb51dataset/video'),\n",
    "    'annotation_path': os.path.join(root, '../datasets/hmdb51dataset/annotation'),\n",
    "    'checkpoints_path': os.path.join(root, 'checkpoints', timestamp),\n",
    "    'history_path': os.path.join(root, 'history', timestamp),\n",
    "    'results_path': os.path.join(root, 'results', timestamp)\n",
    "}\n",
    "if not os.path.exists(location['checkpoints_path']):\n",
    "    os.makedirs(location['checkpoints_path'])\n",
    "\n",
    "if not os.path.exists(location['history_path']):\n",
    "    os.makedirs(location['history_path'])\n",
    "\n",
    "if not os.path.exists(location['results_path']):\n",
    "    os.makedirs(location['results_path'])\n",
    "\n",
    "# Preprocessing dataset\n",
    "transform_train = transforms.Compose([\n",
    "    ToFloatTensorInZeroOne(),\n",
    "    transforms.Resize([256,342]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomCrop(224)\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    ToFloatTensorInZeroOne(),\n",
    "    transforms.Resize([256,342]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "dataset_train_val = torchvision.datasets.HMDB51(\n",
    "    root=location['video_path'],\n",
    "    annotation_path=location['annotation_path'],\n",
    "    frames_per_clip=10,\n",
    "    step_between_clips=5,\n",
    "    train=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "dataset_test = torchvision.datasets.HMDB51(\n",
    "    root=location['video_path'],\n",
    "    annotation_path=location['annotation_path'],\n",
    "    frames_per_clip=10,\n",
    "    step_between_clips=5,\n",
    "    train=False,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "# Train set 52.5%, validation set 17.5%, test set 30%\n",
    "dataset_len = len(dataset_train_val)\n",
    "train_len = math.floor(dataset_len * 0.75)\n",
    "val_len = dataset_len - train_len\n",
    "dataset_train, dataset_val = random_split(dataset_train_val, [train_len, val_len])\n",
    "\n",
    "# Loading dataset\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "loader_val = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "    dataset=dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "train_batches = len(loader_train)\n",
    "val_batches = len(loader_val)\n",
    "test_batches = len(loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
